{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from skimpy.utils.tabdict import TabDict\n",
    "import os\n",
    "import configparser\n",
    "from pytfa.io.json import load_json_model\n",
    "from skimpy.io.yaml import load_yaml_model\n",
    "from skimpy.analysis.oracle.minimum_fluxes import MinFLuxVariable\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../utils\")))\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "\n",
    "UPPER_IX = 1000\n",
    "PHYSIOLOGY = 'WT'\n",
    "\n",
    "# Read configuration from config.ini (single instance, robust path)\n",
    "config = configparser.ConfigParser()\n",
    "config_path = '../src/config.ini'\n",
    "config.read(config_path)\n",
    "\n",
    "# Path to data and model from config, using base_dir\n",
    "base_dir = config['paths']['base_dir']\n",
    "\n",
    "path_to_kmodel = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_kmodel_{PHYSIOLOGY}']))\n",
    "path_to_tmodel = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_tmodel_{PHYSIOLOGY}']))\n",
    "path_to_samples = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_samples_{PHYSIOLOGY}']))\n",
    "path_to_fcc = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_fcc_{PHYSIOLOGY}']))\n",
    "path_to_fcc_df_WT = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_cc_df_WT']))\n",
    "path_to_fcc_df_MUT = os.path.abspath(os.path.join(base_dir, config['paths'][f'path_to_cc_df_MUT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pytfa model\n",
    "print('Loading TFA model from:', path_to_tmodel)\n",
    "tmodel = load_json_model(path_to_tmodel)\n",
    "samples = pd.read_csv(path_to_samples, index_col=0)\n",
    "\n",
    "# Find the producing reactions for each BBB\n",
    "bbb_producing_reactions = {}\n",
    "for bbb in tmodel.reactions.biomass.reactants:\n",
    "    if bbb.id.endswith('_n'):\n",
    "        print(f'Converting {bbb.id} to cytosolic metabolite.')\n",
    "        bbb = tmodel.metabolites.get_by_id(bbb.id[:-2] + '_c')  # Convert to cytosolic metabolite if necessary\n",
    "    # Find the producing reactions of the BBB\n",
    "    if bbb.id in ['h2o_c']:\n",
    "        continue\n",
    "    reactions = [r.id for r in bbb.reactions if r.lower_bound*r.metabolites[bbb] > 0]\n",
    "    bbb_producing_reactions[bbb.id] = reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fccs_for_reactions(ss, rxns, path = path_to_fcc):\n",
    "    '''Get FCCs for all provided reactions at one steady state'''\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    rxn_to_dfs = defaultdict(list)\n",
    "\n",
    "    for i in range(0, 100, 10):\n",
    "        try:\n",
    "            with open(path.format(ss, i, i + 9), 'rb') as f:\n",
    "                fcc = pickle.load(f)\n",
    "            for rxn in rxns:\n",
    "                try:\n",
    "                    slice_df = fcc.slice_by('flux', rxn)\n",
    "                    slice_df.columns = [f'{ss},' + str(col) for col in slice_df.columns]\n",
    "                    rxn_to_dfs[rxn].append(slice_df)\n",
    "                except KeyError:\n",
    "                    continue  # This reaction may not be present\n",
    "        except (FileNotFoundError, OSError):\n",
    "            continue\n",
    "\n",
    "    # Concatenate all slices per reaction\n",
    "    return {\n",
    "        rxn: pd.concat(dfs, axis=1) if dfs else pd.DataFrame()\n",
    "        for rxn, dfs in rxn_to_dfs.items()\n",
    "    }\n",
    "\n",
    "def parallel_get_fccs(max_ss=600, rxns=['biomass'], n_jobs=100):\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    all_results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_fccs_for_reactions)(i, rxns) for i in tqdm(range(max_ss), desc='Loading FCCs')\n",
    "    )\n",
    "    \n",
    "    # Combine results for each reaction\n",
    "    rxn_to_dfs = defaultdict(list)\n",
    "    for result in all_results:\n",
    "        for rxn, df in result.items():\n",
    "            if not df.empty:\n",
    "                rxn_to_dfs[rxn].append(df)\n",
    "\n",
    "    return {\n",
    "        rxn: pd.concat(dfs, axis=1) if dfs else pd.DataFrame()\n",
    "        for rxn, dfs in rxn_to_dfs.items()\n",
    "    }\n",
    "\n",
    "def process_reactions(met, rxn_ids, max_ss=700, remove_outliers=True, path_to_save = path_to_fcc_df_WT):\n",
    "    rxn_ids = list(rxn_ids) \n",
    "    name_for_saving = met+'_producing_reactions'\n",
    "    if os.path.exists(path_to_save.format(name_for_saving)):\n",
    "        print(f\"Skipping {met} as it already exists.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        all_fccs = parallel_get_fccs(max_ss=max_ss, rxns=rxn_ids)\n",
    "        print('Finished loading FCCs')\n",
    "        weights = []\n",
    "        fccs = []\n",
    "        for rxn, df in all_fccs.items():\n",
    "            print(rxn)\n",
    "            if df.empty:\n",
    "                print(f\"No FCC data found for {rxn}\")\n",
    "                continue\n",
    "            # Remove outliers\n",
    "            if remove_outliers:\n",
    "                df = remove_outliers_parallel(df, n_jobs=100)\n",
    "            fccs.append(df.reindex(sorted(df.columns), axis=1))\n",
    "\n",
    "            weights.append(samples.loc[:, tmodel.reactions.get_by_id(rxn).id] - samples.loc[:, tmodel.reactions.get_by_id(rxn).reverse_id])\n",
    "\n",
    "        # Stack all fccs into a 3D numpy array: shape (n_samples, n_rows, n_cols)\n",
    "        fcc_array = np.stack([fcc.values for fcc in fccs])  # shape: (n_samples, n_rows, n_cols)\n",
    "\n",
    "        # Extract column names and index from the first dataframe\n",
    "        columns = fccs[0].columns\n",
    "        index = fccs[0].index\n",
    "        n_rows, n_cols = len(index), len(columns)\n",
    "\n",
    "        # Precompute weights for each column based on 'ss'\n",
    "        final_data = []\n",
    "\n",
    "        for col_idx, col_name in enumerate(columns):\n",
    "            ss = int(col_name.split(',')[0])\n",
    "            col_values = fcc_array[:, :, col_idx]  # shape: (n_samples, n_rows)\n",
    "            \n",
    "            # Extract weights for this ss\n",
    "            w = np.array([weight[ss] for weight in weights])  # shape: (n_samples,)\n",
    "            \n",
    "            numerator = np.tensordot(w, col_values, axes=(0, 0))  # shape: (n_rows,)\n",
    "            denominator = w.sum()\n",
    "            \n",
    "            if denominator != 0:\n",
    "                final_data.append(numerator / denominator)\n",
    "            else:\n",
    "                final_data.append(np.zeros(n_rows))\n",
    "\n",
    "        # Stack column-wise and convert to DataFrame\n",
    "        final_fcc = pd.DataFrame(np.column_stack(final_data), columns=columns, index=index)\n",
    "\n",
    "        # Save the final FCCs for this metabolite\n",
    "        final_fcc.to_csv(path_to_save.format(name_for_saving))\n",
    "        print(f\"Finished processing {met} with {len(rxn_ids)} reactions.\")\n",
    "\n",
    "        # Delete the fccs list to free memory\n",
    "        del fccs\n",
    "        del fcc_array\n",
    "        del final_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for reactions {rxn_ids}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_fccs_WT = pd.DataFrame(columns=bbb_producing_reactions.keys())\n",
    "from tqdm import tqdm\n",
    "for met, rxns in bbb_producing_reactions.items():\n",
    "    name_for_saving = 'synthesis_rate_' + met\n",
    "    filename = path_to_fcc_df_WT.format(name_for_saving)\n",
    "\n",
    "    # First, get column names to define dtypes\n",
    "    total_flux_df_cols = pd.read_csv(filename, index_col=0, nrows=0).columns\n",
    "    dtype_dict = {col: 'float32' for col in total_flux_df_cols}\n",
    "    dtype_dict['model_ix'] = 'string'\n",
    "\n",
    "    # Count total number of lines for progress bar\n",
    "    total_lines = sum(1 for _ in open(filename)) - 1  # Subtract header\n",
    "    chunksize = 1000\n",
    "\n",
    "    # Read in chunks using index_col=0 to preserve the original index\n",
    "    chunk_iter = pd.read_csv(filename, chunksize=chunksize, index_col=0, dtype=dtype_dict)\n",
    "\n",
    "    df_chunks = []\n",
    "    for chunk in tqdm(chunk_iter, total=total_lines // chunksize + 1, desc=\"Loading CSV\"):\n",
    "        df_chunks.append(chunk)\n",
    "\n",
    "    # Concatenate chunks WITHOUT ignore_index so original index is preserved\n",
    "    total_flux_df = pd.concat(df_chunks)\n",
    "\n",
    "    # Keep the average FCCs\n",
    "    average_fccs_WT[met] = total_flux_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_fccs_MUT = pd.DataFrame(columns=bbb_producing_reactions.keys())\n",
    "from tqdm import tqdm\n",
    "for met, rxns in bbb_producing_reactions.items():\n",
    "    name_for_saving = 'synthesis_rate_' + met\n",
    "    filename = path_to_fcc_df_MUT.format(name_for_saving)\n",
    "\n",
    "    # First, get column names to define dtypes\n",
    "    total_flux_df_cols = pd.read_csv(filename, index_col=0, nrows=0).columns\n",
    "    dtype_dict = {col: 'float32' for col in total_flux_df_cols}\n",
    "    dtype_dict['model_ix'] = 'string'\n",
    "\n",
    "    # Count total number of lines for progress bar\n",
    "    total_lines = sum(1 for _ in open(filename)) - 1  # Subtract header\n",
    "    chunksize = 1000\n",
    "\n",
    "    # Read in chunks using index_col=0 to preserve the original index\n",
    "    chunk_iter = pd.read_csv(filename, chunksize=chunksize, index_col=0, dtype=dtype_dict)\n",
    "\n",
    "    df_chunks = []\n",
    "    for chunk in tqdm(chunk_iter, total=total_lines // chunksize + 1, desc=\"Loading CSV\"):\n",
    "        df_chunks.append(chunk)\n",
    "\n",
    "    # Concatenate chunks WITHOUT ignore_index so original index is preserved\n",
    "    total_flux_df = pd.concat(df_chunks)\n",
    "\n",
    "    # Keep the average FCCs\n",
    "    average_fccs_MUT[met] = total_flux_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns of essential amino acids and non essential amino acids\n",
    "group_df = pd.DataFrame([\n",
    "    (\"his_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"ile_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"leu_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"lys_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"met_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"phe_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"thr_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"trp_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"val_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"ala_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"arg_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"asn_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"asp_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"cys_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"gln_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"glu_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"gly_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"pro_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"ser_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"tyr_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"atp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"ctp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"gtp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"utp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"datp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dctp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dgtp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dttp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"chsterol_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"clpn_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pail_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pchol_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pe_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pglyc_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"ps_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"sphmyln_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"g6p_c\", \"Carbohydrate\", \"\")\n",
    "], columns=[\"Metabolite\", \"Category\", \"Subcategory\"])\n",
    "\n",
    "average_fccs_MUT = average_fccs_MUT.drop(columns=group_df[group_df['Subcategory'] == 'Essential']['Metabolite'].tolist())\n",
    "average_fccs_MUT = average_fccs_MUT.drop(columns=group_df[group_df['Subcategory'] == 'Non-Essential']['Metabolite'].tolist())\n",
    "\n",
    "average_fccs_WT = average_fccs_WT.drop(columns=group_df[group_df['Subcategory'] == 'Essential']['Metabolite'].tolist())\n",
    "average_fccs_WT = average_fccs_WT.drop(columns=group_df[group_df['Subcategory'] == 'Non-Essential']['Metabolite'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Subset and clean index for average_fccs\n",
    "mca_targets = [\n",
    " 'vmax_forward_TRIOK', 'vmax_forward_MI1PP', 'vmax_forward_PPAP', 'vmax_forward_r0301', 'vmax_forward_METAT',\n",
    " 'vmax_forward_3DSPHR', 'vmax_forward_TMDS', 'vmax_forward_SERPT', 'vmax_forward_HMR_7748', 'vmax_forward_PSP_L',\n",
    " 'vmax_forward_NADH2_u10mi', 'vmax_forward_DGK1', 'vmax_forward_NTD1', 'vmax_forward_r0354', 'vmax_forward_ADSS',\n",
    " 'vmax_forward_r0178', 'vmax_forward_IMPD', 'vmax_forward_r0179', 'vmax_forward_PGI', 'vmax_forward_r0474',\n",
    " 'vmax_forward_ICDHyrm', 'vmax_forward_GMPS2', 'vmax_forward_UMPK2', 'vmax_forward_ICDHxm', 'vmax_forward_URIK1',\n",
    " 'vmax_forward_CYTK1', 'vmax_forward_HMR_4343', 'vmax_forward_r0426'\n",
    "]\n",
    "\n",
    "\n",
    "sorted_names = ['TRIOK', 'HMR_7748', 'r0354', 'PGI', 'ICDHyrm', 'ICDHxm', 'r0426', 'PPAP', 'r0301', '3DSPHR', 'SERPT',\n",
    "                 'METAT', 'PSP_L', 'r0178', 'r0179', 'TMDS', 'NTD1', 'DGK1', 'ADSS', 'IMPD', 'r0474', 'URIK1', 'GMPS2',\n",
    "                   'HMR_4343', 'CYTK1', 'UMPK2', 'MI1PP', 'NADH2_u10mi']\n",
    "\n",
    "data_to_plot_MUT = average_fccs_MUT.loc[set(mca_targets)]\n",
    "data_to_plot_MUT.index = [i.split('vmax_forward_',)[1] for i in data_to_plot_MUT.index]\n",
    "data_to_plot_MUT = data_to_plot_MUT.loc[(sorted_names)]\n",
    "# Replace NADH2_u10mi in the index with MComplex1\n",
    "data_to_plot_MUT = data_to_plot_MUT.rename(index={\"NADH2_u10mi\": \"MComplex1\"})\n",
    "\n",
    "data_to_plot_WT = average_fccs_WT.loc[set(mca_targets)]\n",
    "data_to_plot_WT.index = [i.split('vmax_forward_',)[1] for i in data_to_plot_WT.index]\n",
    "data_to_plot_WT = data_to_plot_WT.loc[(sorted_names)]\n",
    "# Replace NADH2_u10mi in the index with MComplex1\n",
    "data_to_plot_WT = data_to_plot_WT.rename(index={\"NADH2_u10mi\": \"MComplex1\"})\n",
    "\n",
    "# Create the mapping from metabolite to subcategory\n",
    "group_df = pd.DataFrame([\n",
    "    (\"his_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"ile_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"leu_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"lys_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"met_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"phe_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"thr_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"trp_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"val_L_c\", \"Amino Acid\", \"Essential\"),\n",
    "    (\"ala_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"arg_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"asn_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"asp_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"cys_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"gln_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"glu_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"gly_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"pro_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"ser_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"tyr_L_c\", \"Amino Acid\", \"Non-Essential\"),\n",
    "    (\"atp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"ctp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"gtp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"utp_c\", \"Ribonucleotide\", \"Nucleotides\"),\n",
    "    (\"datp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dctp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dgtp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"dttp_c\", \"Deoxyribonucleotide\", \"Deoxynucleotides\"),\n",
    "    (\"chsterol_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"clpn_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pail_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pchol_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pe_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"pglyc_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"ps_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"sphmyln_hs_c\", \"Lipid\", \"Lipids\"),\n",
    "    (\"g6p_c\", \"Carbohydrate\", \"\")\n",
    "], columns=[\"Metabolite\", \"Category\", \"Subcategory\"])\n",
    "\n",
    "# Reorder columns by subcategory\n",
    "met_to_subcat = group_df.set_index(\"Metabolite\")[\"Subcategory\"].to_dict()\n",
    "existing_columns = [col for col in data_to_plot_WT.columns if col in met_to_subcat]\n",
    "\n",
    "subcategory_order = [\n",
    "    'Essential',\n",
    "    'Deoxynucleotides',\n",
    "    'Nucleotides',\n",
    "    \"Non-Essential\",\n",
    "    \"Lipids\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "sorted_columns = sorted(\n",
    "    existing_columns,\n",
    "    key=lambda x: (subcategory_order.index(met_to_subcat[x]), x)\n",
    ")\n",
    "data_to_plot_WT = data_to_plot_WT[sorted_columns]\n",
    "data_to_plot_MUT = data_to_plot_MUT[sorted_columns]\n",
    "\n",
    "# Prepare subcategory boundaries for bracket plotting\n",
    "subcat_to_indices = OrderedDict()\n",
    "for i, col in enumerate(data_to_plot_WT.columns):\n",
    "    subcat = met_to_subcat[col]\n",
    "    if subcat not in subcat_to_indices:\n",
    "        subcat_to_indices[subcat] = [i, i]\n",
    "    else:\n",
    "        subcat_to_indices[subcat][1] = i\n",
    "        \n",
    "# Function to add brackets to an axis\n",
    "def add_brackets(ax, subcat_to_indices, met_to_subcat):\n",
    "    # Positions just above the heatmap axes area (negative y puts them above x-ticks)\n",
    "    bracket_y = -2\n",
    "    text_y = -2.5\n",
    "    for label, (start, end) in subcat_to_indices.items():\n",
    "        if label == '':\n",
    "            continue\n",
    "        if label == 'Essential':\n",
    "            label = 'Essential AAs'\n",
    "        elif label == 'Non-Essential':\n",
    "            label = 'Non-Essential AAs'\n",
    "        # Horizontal line spanning the subcategory\n",
    "        ax.hlines(bracket_y, start + 0.1, end + 1 - 0.1, color='black', linewidth=1.5, clip_on=False)\n",
    "        # Vertical tips\n",
    "        ax.vlines([start + 0.1, end + 1 - 0.1], bracket_y, bracket_y + 1, color='black', linewidth=1.5, clip_on=False)\n",
    "        # Label centered over the span\n",
    "        ax.text((start + end + 1) / 2, text_y, label,\n",
    "                ha='center', va='bottom', fontsize=20, rotation=45, clip_on=False)\n",
    "\n",
    "# Compute vertical separator positions between adjacent subcategories (dashed lines)\n",
    "# This avoids hardcoding indices and adapts to whatever columns are present\n",
    "separator_positions = []\n",
    "last_end = None\n",
    "ordered_blocks = list(subcat_to_indices.items())\n",
    "for idx, (label, (start, end)) in enumerate(ordered_blocks):\n",
    "    if label == '':\n",
    "        continue\n",
    "    if idx > 0:\n",
    "        # Separator goes at the left edge of this block\n",
    "        separator_positions.append(start)\n",
    "\n",
    "# Plot settings\n",
    "VMIN, VMAX = -2, 2  # keep the same dynamic range as the previous figure for comparability\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "fig = plt.figure(figsize=(35, 20))\n",
    "ax1 = plt.subplot2grid((1, 25), (0, 0), colspan=10)   # WT\n",
    "ax2 = plt.subplot2grid((1, 25), (0, 11), colspan=10)  # MUT\n",
    "\n",
    "# Left heatmap (WT)\n",
    "im_wt = sns.heatmap(\n",
    "    data_to_plot_WT,\n",
    "    cmap='seismic',\n",
    "    center=0,\n",
    "    vmin=VMIN,\n",
    "    vmax=VMAX,\n",
    "    cbar=False,\n",
    "    ax=ax1,\n",
    "    square=True\n",
    ")\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Enzymes', fontsize=26)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "add_brackets(ax1, subcat_to_indices, met_to_subcat)\n",
    "\n",
    "# Right heatmap (MUT)\n",
    "im_mut = sns.heatmap(\n",
    "    data_to_plot_MUT,\n",
    "    cmap='seismic',\n",
    "    center=0,\n",
    "    vmin=VMIN,\n",
    "    vmax=VMAX,\n",
    "    cbar=False,\n",
    "    ax=ax2,\n",
    "    square=True\n",
    ")\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('')\n",
    "ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "add_brackets(ax2, subcat_to_indices, met_to_subcat)\n",
    "\n",
    "\n",
    "# Draw dashed separators at subcategory boundaries\n",
    "for x in separator_positions:\n",
    "    ax1.axvline(x=x, color='black', linewidth=1, linestyle='--')\n",
    "    ax2.axvline(x=x, color='black', linewidth=1, linestyle='--')\n",
    "\n",
    "# Inset colorbar to the right of the MUT heatmap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "cax = inset_axes(\n",
    "    ax2,\n",
    "    width=\"6%\",\n",
    "    height=\"30%\",\n",
    "    loc='center left',\n",
    "    bbox_to_anchor=(1.05, 0.35, 1, 1),\n",
    "    bbox_transform=ax2.transAxes,\n",
    "    borderpad=0\n",
    ")\n",
    "# Use the right heatmap's mappable for the colorbar\n",
    "cbar = plt.colorbar(im_mut.collections[0], cax=cax)\n",
    "cbar.set_label('Effect on each biomass precursor', fontsize=20, labelpad=6)\n",
    "cbar.set_ticks([VMIN, 0, VMAX])\n",
    "cbar.set_ticklabels([f'< {VMIN}', '0', f'> {VMAX}'])\n",
    "cbar.ax.tick_params(labelsize=20, length=3, width=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
