{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimpy.analysis.oracle import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"../../../../NRAplus/NRAplus\") # Adds higher directory to python modules path.\n",
    "sys.path.append('../')\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import plotly.graph_objects as go\n",
    "import kaleido\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from utils.nra_save_custom_json import load_json_nra_model\n",
    "from pytfa.io.json import load_json_model\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.abspath('../src/config.ini'))\n",
    "base_dir = config['paths']['base_dir']\n",
    "path_to_nra_model = os.path.join(base_dir, config['paths']['path_to_nra_model'])\n",
    "path_to_solutions = os.path.join(base_dir, config['paths']['path_to_solutions'])\n",
    "path_to_essential_enzymes = os.path.join(base_dir, config['paths']['path_to_essential_enzymes'])\n",
    "path_to_tflink_database = os.path.join(base_dir, config['paths']['path_to_tflink_database'])\n",
    "path_to_recon_model = os.path.join(base_dir, config['paths']['path_to_recon_model'])\n",
    "path_to_gene_to_uniprot_mapping = os.path.join(base_dir, config['paths']['path_to_gene_to_uniprot_mapping'])\n",
    "path_to_essential_enzymes_to_TFs_mapping = os.path.join(base_dir, config['paths']['path_to_essential_enzymes_to_TFs_mapping'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nra_model = load_json_nra_model(path_to_nra_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the essential enzymes txt file\n",
    "with open(path_to_essential_enzymes, 'r') as f:\n",
    "    essential_enzymes_list = [fline.strip() for fline in f.readlines()]\n",
    "\n",
    "# Load the reference solution\n",
    "sol = pd.read_csv(path_to_solutions.format('250'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a dataframe the enzyme name if it is up or down regulated and the subsystem it belongs to\n",
    "essential_enzymes = pd.DataFrame(columns=['Enzyme', 'Subsystem', 'Regulation'])\n",
    "for enz in essential_enzymes_list:\n",
    "    var_up = sol.loc['EUU_' + enz].values[0]\n",
    "    var_down = sol.loc['EDU_' + enz].values[0]\n",
    "    if np.isclose(var_up, 1.0):\n",
    "        regulation = 'up'\n",
    "    elif np.isclose(var_down, 1.0):\n",
    "        regulation = 'down'\n",
    "    else:\n",
    "        raise ValueError(f\"Enzyme {enz} is not up or down regulated\")\n",
    "\n",
    "    rxn = nra_model.reactions.get_by_id(enz)\n",
    "    sub = rxn.subsystem\n",
    "    essential_enzymes = essential_enzymes.append({'Enzyme': enz, 'Subsystem': sub, 'Regulation': regulation}, ignore_index=True)\n",
    "\n",
    "# Set the index to the enzyme name\n",
    "essential_enzymes.set_index('Enzyme', inplace=True)\n",
    "\n",
    "# Print how many enzymes are up or down regulated \n",
    "print(f\"Number of down regulated enzymes: {len(essential_enzymes[essential_enzymes['Regulation'] == 'down'])}\")\n",
    "print(f\"Number of up regulated enzymes: {len(essential_enzymes[essential_enzymes['Regulation'] == 'up'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for high-impact journals\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Count number of up- and down-regulated enzymes per subsystem\n",
    "df = essential_enzymes.copy()\n",
    "\n",
    "# Rename the long subsystem\n",
    "df['Subsystem'] = df['Subsystem'].replace(\n",
    "    'Glycine, serine, alanine, and threonine metabolism',\n",
    "    'Gly/Se/rAla/Theo metabolism'\n",
    ")\n",
    "\n",
    "# Lump subsystems with fewer than 8 enzymes into \"Other\"\n",
    "subsystem_counts = df['Subsystem'].value_counts()\n",
    "rare_subsystems = subsystem_counts[subsystem_counts < 8].index\n",
    "df['Subsystem'] = df['Subsystem'].replace(rare_subsystems, 'Other')\n",
    "\n",
    "# Group and count\n",
    "count_df = df.groupby(['Subsystem', 'Regulation']).size().reset_index(name='Count')\n",
    "\n",
    "# Ensure 'Other' is at the end\n",
    "subsystem_order = [s for s in count_df['Subsystem'].unique() if s != 'Other']\n",
    "subsystem_order.sort()\n",
    "subsystem_order.append('Other')\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72']  # Blue and burgundy\n",
    "\n",
    "# Calculate total essential enzymes per subsystem and sort\n",
    "totals = [\n",
    "    \"Gly/Se/rAla/Theo metabolism\",\n",
    "    \"Arginine and proline metabolism\",\n",
    "    \"Glutamate metabolism\",\n",
    "    \"Glycolysis/gluconeogenesis\",\n",
    "    \"Pentose phosphate pathway\",\n",
    "    \"Citric acid cycle\",\n",
    "    \"Urea cycle\",\n",
    "    \"Pyrimidine synthesis\",\n",
    "    \"NAD metabolism\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 6))  # taller than it is wide\n",
    "sns.barplot(\n",
    "    data=count_df,\n",
    "    y='Subsystem',\n",
    "    x='Count',\n",
    "    hue='Regulation',\n",
    "    order=totals,\n",
    "    palette=colors,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "# swap labels\n",
    "ax.set_ylabel('Metabolic Pathway', fontsize=12)\n",
    "ax.set_xlabel('Number of Essential Enzymes', fontsize=12)\n",
    "\n",
    "# keep pathway names horizontal\n",
    "plt.yticks(rotation=0)\n",
    "ax.tick_params(axis='y', which='both', length=0)  # ensure no little stubs\n",
    "\n",
    "\n",
    "# move legend to the right, outside the axes\n",
    "legend = ax.legend(\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(1.0, 1.0),\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# rename legend entries\n",
    "legend.texts[0].set_text('Downregulated')\n",
    "legend.texts[1].set_text('Upregulated')\n",
    "\n",
    "# remove top and right spines\n",
    "sns.despine(ax=ax, top=True, right=True)\n",
    "\n",
    "# give a bit of extra right margin so the legend isnâ€™t cut off\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "# plt.savefig(\n",
    "#     '../../results/physiology_comparison/essential_enzymes_subystems_horizontal_legend.pdf',\n",
    "#     dpi=300,\n",
    "#     transparent=True,\n",
    "#     bbox_inches='tight',\n",
    "#     pad_inches=0.1\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Figure 5E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLink database\n",
    "tf_link = pd.read_csv(path_to_tflink_database, sep='\\t')\n",
    "\n",
    "from cobra.io.json import load_json_model # FBA type import\n",
    "recon = load_json_model(path_to_recon_model)\n",
    "\n",
    "# Load gene to UNIPROT mapping\n",
    "gene_uniprot_mapping = pd.read_csv(path_to_gene_to_uniprot_mapping, sep='\\t')\n",
    "\n",
    "# The first column should be a string\n",
    "gene_uniprot_mapping.iloc[:, 0] = gene_uniprot_mapping.iloc[:, 0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_total = tf_link[tf_link['Name.TF']=='BRCA1']\n",
    "\n",
    "# Find which genes from recon3d are targeted by BRCA1\n",
    "genes_identified = []\n",
    "for ll in gene_uniprot_mapping.index:\n",
    "    ncbi = gene_uniprot_mapping.loc[ll,'gene_number'].split('.')[0]\n",
    "    if str(ncbi) in tf_link.loc[:,'NCBI.GeneID.Target'].values:\n",
    "        row = tf_link[tf_link.loc[:,'NCBI.GeneID.Target'] == str(ncbi)]\n",
    "        print(row)\n",
    "        genes_identified.append(row)\n",
    "\n",
    "# Make into a datframe\n",
    "genes_identified = pd.concat(genes_identified, ignore_index=True)\n",
    "genes_identified = genes_identified.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# Now we go from those TFs as targets to BRCA1 as a TF\n",
    "tfs_identified = []\n",
    "for ll in genes_identified.index:\n",
    "    tf_name = genes_identified.loc[ll,'Name.TF']\n",
    "    if tf_name in brca1_total.loc[:,'Name.TF'].values:\n",
    "        row = brca1_total[brca1_total.loc[:,'Name.TF'] == tf_name]\n",
    "        print(row)\n",
    "        tfs_identified.append(row)\n",
    "\n",
    "# Make into a datframe\n",
    "tfs_identified = pd.concat(tfs_identified, ignore_index=True)\n",
    "tfs_identified = tfs_identified.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# Go back to genes through the TFS identified in tfs_identified\n",
    "brca1_specific_genes = []\n",
    "for ll in tfs_identified.index:\n",
    "    tf_name = tfs_identified.loc[ll,'Name.Target']\n",
    "    if tf_name in genes_identified.loc[:,'Name.TF'].values:\n",
    "        row = genes_identified[genes_identified.loc[:,'Name.TF'] == tf_name]\n",
    "        print(row)\n",
    "        brca1_specific_genes.append(row)\n",
    "\n",
    "# Make into a datframe\n",
    "brca1_specific_genes = pd.concat(brca1_specific_genes, ignore_index=True)\n",
    "brca1_specific_genes = brca1_specific_genes.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enz = []\n",
    "for i in recon.genes:\n",
    "    name = i.id.split('_')[0]\n",
    "    if name in brca1_specific_genes.loc[:,'NCBI.GeneID.Target'].values:\n",
    "        print(name)\n",
    "\n",
    "        for rxn in i.reactions:\n",
    "            if rxn.id not in enz:\n",
    "                enz.append(rxn.id)\n",
    "                print(rxn.id)\n",
    "print(f'Total number of reactions associated with BRCA1: {len(enz)}')\n",
    "\n",
    "# Do the same for the small model\n",
    "enz_small = []\n",
    "for i in nra_model.genes:\n",
    "    name = i.id.split('.',1)[0]\n",
    "    if name in brca1_specific_genes.loc[:,'NCBI.GeneID.Target'].values:\n",
    "        print(name)\n",
    "\n",
    "        for rxn in i.reactions:\n",
    "            if rxn.id not in enz_small:\n",
    "                enz_small.append(rxn.id)\n",
    "                print(rxn.id)\n",
    "print(f'Total number of reactions associated with BRCA1 in the small model: {len(enz_small)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each essential enzyme find the genes ascosiated with it\n",
    "essential_enzymes['genes'] = pd.Series(dtype=object) # Initialize with object dtype\n",
    "essential_enzymes['uniprot_genes'] = pd.Series(dtype=object) # Initialize with object dtype\n",
    "essential_enzymes['hugo_genes'] = pd.Series(dtype=object) # Initialize with object dtype\n",
    "\n",
    "# Iterate through each essential enzyme (reaction ID)\n",
    "for enz in essential_enzymes.index:\n",
    "    rxn = nra_model.reactions.get_by_id(enz)\n",
    "    genes = [g.id for g in rxn.genes]\n",
    "\n",
    "    if genes:\n",
    "        # Join multiple gene IDs with ';'\n",
    "        essential_enzymes.at[enz, 'genes'] = ';'.join(genes)\n",
    "        # Find the corresponding uniprot IDs\n",
    "        uniprot_ids = []\n",
    "        for gene in genes:\n",
    "            if gene in gene_uniprot_mapping['gene_number'].values:\n",
    "                uniprot_id = gene_uniprot_mapping.loc[gene_uniprot_mapping['gene_number'] == gene, 'uniprot_gname'].values[0]\n",
    "                if uniprot_id not in uniprot_ids and not pd.isna(uniprot_id):\n",
    "                    uniprot_ids.append(uniprot_id)\n",
    "        # Join multiple uniprot IDs with ';'\n",
    "        if uniprot_ids:\n",
    "            essential_enzymes.at[enz, 'uniprot_genes'] = ';'.join(uniprot_ids)\n",
    "        else:\n",
    "            # Keep as NaN if no uniprot IDs are found\n",
    "            essential_enzymes.at[enz, 'uniprot_genes'] = np.nan\n",
    "\n",
    "        # Find the corresponding HUGO gene names\n",
    "        hugo_genes = []\n",
    "        for gene in genes:\n",
    "            if gene in gene_uniprot_mapping['gene_number'].values:\n",
    "                hugo_id = gene_uniprot_mapping.loc[gene_uniprot_mapping['gene_number'] == gene, 'symbol'].values[0]\n",
    "                if hugo_id not in hugo_genes and not pd.isna(hugo_id):\n",
    "                    hugo_genes.append(hugo_id)\n",
    "        # Join multiple HUGO gene names with ';'\n",
    "        if hugo_genes:\n",
    "            essential_enzymes.at[enz, 'hugo_genes'] = ';'.join(hugo_genes)\n",
    "        else:\n",
    "            # Keep as NaN if no HUGO gene names are found\n",
    "            essential_enzymes.at[enz, 'hugo_genes'] = np.nan\n",
    "    else:\n",
    "        # Keep as NaN if no genes are associated\n",
    "        essential_enzymes.at[enz, 'genes'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TFLink returns more TF hits for the essential enzymes\n",
    "essential_enzymes['TF_link'] = pd.Series(dtype=object) # Initialize with object dtype\n",
    "\n",
    "for enz in essential_enzymes.index:\n",
    "    uniprot_ids = essential_enzymes.loc[enz, 'uniprot_genes']\n",
    "    if not pd.isna(uniprot_ids):\n",
    "        uniprot_ids_split = uniprot_ids.split(';')\n",
    "        tf_hits = []\n",
    "        for uniprot_id in uniprot_ids_split:\n",
    "            tf_matches = tf_link.loc[tf_link['UniprotID.Target'] == uniprot_id, 'Name.TF']\n",
    "            if not tf_matches.empty:\n",
    "                tf_hits.extend(tf_matches.unique())\n",
    "        if tf_hits:\n",
    "            essential_enzymes.at[enz, 'TF_link'] = ';'.join(tf_hits)\n",
    "        else:\n",
    "            essential_enzymes.at[enz, 'TF_link'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many unique genes are there\n",
    "gene_list = []\n",
    "for i in essential_enzymes['genes'].dropna():\n",
    "    genes = i.split(';')\n",
    "    for gene in genes:\n",
    "        gene_list.append(gene)\n",
    "print(f\"{essential_enzymes.genes.notna().sum()}/{len(essential_enzymes)} essential enzymes have genes associated with them\")\n",
    "print(f\"Number of genes: {len(gene_list)}\")\n",
    "print(f\"Number of unique genes: {len(set(gene_list))}\\n\")\n",
    "\n",
    "# Find how many unique TFs are there for TF_link\n",
    "tfs_list_tf_link = []\n",
    "for i in essential_enzymes['TF_link'].dropna():\n",
    "    tfs = i.split(';')\n",
    "    for tf in tfs:\n",
    "        tfs_list_tf_link.append(tf)\n",
    "print(f\"{essential_enzymes.TF_link.notna().sum()}/{len(essential_enzymes)} essential enzymes have TF_link TFs associated with them\")\n",
    "print(f\"Number of TFs: {len(tfs_list_tf_link)}\")\n",
    "print(f\"Number of unique TFs: {len(set(tfs_list_tf_link))}\\n\")\n",
    "\n",
    "# Find the unique genes that are associated with the TF_link TFs\n",
    "genes_tfs_tf_link = []\n",
    "for i in essential_enzymes[essential_enzymes.TF_link.notna()].genes:\n",
    "    genes = i.split(';')\n",
    "    for gene in genes:\n",
    "        genes_tfs_tf_link.append(gene)\n",
    "print(f\"Number of genes associated with TF_link TFs: {len(genes_tfs_tf_link)}\")\n",
    "print(f\"Number of unique genes associated with TF_link TFs: {len(set(genes_tfs_tf_link))}\\n\")\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "print(f'In total {essential_enzymes.TF_link.notna().sum()}/{len(essential_enzymes)} essential enzymes are connected to {len(set(genes_tfs_tf_link))} unique genes and are regulated by {len(set(tfs_list_tf_link))} unique TFs (TF_link)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_enzymes.to_csv(path_to_essential_enzymes_to_TFs_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('../../results/physiology_comparison/essential_enzymes_with_genes_and_TFs.csv')\n",
    "\n",
    "# Drop rows missing TF or gene\n",
    "df_filtered = df.dropna(subset=['TF_link', 'hugo_genes'])\n",
    "\n",
    "# Split semicolon-separated values\n",
    "df_filtered['hugo_genes'] = df_filtered['hugo_genes'].astype(str).str.split(';')\n",
    "df_filtered['TF_link'] = df_filtered['TF_link'].astype(str).str.split(';')\n",
    "\n",
    "# Explode all combinations\n",
    "df_exploded = df_filtered.explode('hugo_genes').explode('TF_link')\n",
    "df_exploded['hugo_genes'] = df_exploded['hugo_genes'].str.strip()\n",
    "df_exploded['TF_link'] = df_exploded['TF_link'].str.strip()\n",
    "\n",
    "# !!! IMPORTANT: Apply NADH2_u10mi merge before filtering to top TFs if we want this aggregated enzyme\n",
    "# to be considered in the overall flow calculation for top TFs or relevant enzymes.\n",
    "# If the merge is done later, the 'Enzyme' column for NADH2_u10mi might still be the original for earlier steps.\n",
    "# Let's keep it here as it was.\n",
    "df_exploded['hugo_genes'] = df_exploded.apply(\n",
    "    lambda row: 'NADH2_u10mi genes' if row['Enzyme'] == 'NADH2_u10mi' else row['hugo_genes'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter to top 5 most common TFs based on all original connections\n",
    "top_5_tfs_all_connections = df_exploded['TF_link'].value_counts().nlargest(5).index.tolist()\n",
    "top_5_tfs_all_connections = ['SP1', 'MYC', 'CREB1', 'NFKB1', 'AR', 'HIF1A', 'STAT1', 'EGR1', 'TP53']  # Use the same top TFs as before\n",
    "df_top = df_exploded[df_exploded['TF_link'].isin(top_5_tfs_all_connections)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Filter relevant columns and remove rows without Subsystem info\n",
    "df_top.dropna(subset=['Subsystem'], inplace=True)\n",
    "\n",
    "\n",
    "# --- REVISED LOGIC FOR ORDERING NODES BY FLOW ---\n",
    "\n",
    "# 1. Prepare link dataframes with calculated 'value' for sorting\n",
    "#    These are the actual links that will be drawn, so their 'value' is what we sort by.\n",
    "\n",
    "# TF -> Enzyme links\n",
    "tf_enzyme_flows = df_top.groupby(['TF_link', 'Enzyme']).size().reset_index(name='value')\n",
    "\n",
    "# Enzyme -> Subsystem links\n",
    "enzyme_subsystem_flows = df_top.groupby(['Enzyme', 'Subsystem']).size().reset_index(name='value')\n",
    "\n",
    "# 2. Calculate total 'flow' for each node type (what's coming into it or leaving it)\n",
    "\n",
    "# TFs: Sort by their total outgoing flow to enzymes (which is their primary contribution)\n",
    "tf_outgoing_flow = tf_enzyme_flows.groupby('TF_link')['value'].sum().sort_values(ascending=False)\n",
    "sorted_tfs = tf_outgoing_flow.index.tolist()\n",
    "\n",
    "# Enzymes: Sort by their total incoming flow from TFs.\n",
    "# This aligns with \"inputs in each layer ... biggest inflow to smallest\".\n",
    "# NADH2_u10mi is already handled in df_exploded before this step.\n",
    "enzyme_inflow_sum = tf_enzyme_flows.groupby('Enzyme')['value'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Ensure only enzymes present in the filtered df_top are considered\n",
    "relevant_enzymes_in_df_top = df_top['Enzyme'].unique().tolist()\n",
    "# Filter enzyme_inflow_sum to include only relevant enzymes\n",
    "sorted_enzymes = [e for e in enzyme_inflow_sum.index.tolist() if e in relevant_enzymes_in_df_top]\n",
    "\n",
    "\n",
    "# Subsystems: Sort by their total incoming flow from Enzymes.\n",
    "subsystem_inflow_sum = enzyme_subsystem_flows.groupby('Subsystem')['value'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Ensure only subsystems present in the filtered df_top are considered\n",
    "relevant_subsystems_in_df_top = df_top['Subsystem'].unique().tolist()\n",
    "# Filter subsystem_inflow_sum to include only relevant subsystems\n",
    "sorted_subsystems = [s for s in subsystem_inflow_sum.index.tolist() if s in relevant_subsystems_in_df_top]\n",
    "\n",
    "\n",
    "# 3. Combine all nodes in the desired order for Sankey\n",
    "# This order is CRITICAL for Plotly's rendering.\n",
    "nodes = sorted_tfs + sorted_enzymes + sorted_subsystems\n",
    "node_indices = {name: i for i, name in enumerate(nodes)}\n",
    "\n",
    "# --- END REVISED LOGIC ---\n",
    "\n",
    "# Build Sankey links using the prepared flow dataframes\n",
    "source, target, value = [], [], []\n",
    "\n",
    "# TF â†’ Enzyme links\n",
    "for _, row in tf_enzyme_flows.iterrows():\n",
    "    # Only add links if both source and target nodes exist in our final `nodes` list\n",
    "    if row['TF_link'] in node_indices and row['Enzyme'] in node_indices:\n",
    "        source.append(node_indices[row['TF_link']])\n",
    "        target.append(node_indices[row['Enzyme']])\n",
    "        value.append(row['value'])\n",
    "\n",
    "# Enzyme â†’ Subsystem links\n",
    "for _, row in enzyme_subsystem_flows.iterrows():\n",
    "    # Only add links if both source and target nodes exist in our final `nodes` list\n",
    "    if row['Enzyme'] in node_indices and row['Subsystem'] in node_indices:\n",
    "        source.append(node_indices[row['Enzyme']])\n",
    "        target.append(node_indices[row['Subsystem']])\n",
    "        value.append(row['value'])\n",
    "\n",
    "\n",
    "# Node colors (using the new sorted lists for checks)\n",
    "node_colors = []\n",
    "for node in nodes:\n",
    "    if node in sorted_tfs:\n",
    "        node_colors.append('rgba(255, 127, 14, 0.8)')  # TF = orange\n",
    "    elif node in sorted_enzymes:\n",
    "        node_colors.append('rgba(44, 160, 44, 0.8)') # Enzyme = green\n",
    "    else: # Subsystem\n",
    "        node_colors.append('rgba(31, 119, 180, 0.8)') # Subsystem = blue\n",
    "\n",
    "# Link colors by source type (using the new sorted lists for checks)\n",
    "link_colors = []\n",
    "for s_idx in source: # s_idx is the index in the 'nodes' list\n",
    "    if nodes[s_idx] in sorted_tfs:\n",
    "        link_colors.append('rgba(255, 127, 14, 0.3)')  # TFâ†’Enzyme (orange-ish)\n",
    "    elif nodes[s_idx] in sorted_enzymes:\n",
    "        link_colors.append('rgba(44, 160, 44, 0.3)')   # Enzymeâ†’Subsystem (green-ish)\n",
    "\n",
    "# Create Sankey plot\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=20,\n",
    "        thickness=25,\n",
    "        line=dict(color=\"black\", width=0.8),\n",
    "        label=nodes, # This is the crucial part for ordering\n",
    "        color=node_colors,\n",
    "        hovertemplate='<b>%{label}</b><br>Total flow: %{value}<extra></extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors,\n",
    "        hovertemplate='<b>%{source.label}</b> â†’ <b>%{target.label}</b><br>Flow: %{value}<extra></extra>'\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"\",\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 16, 'family': 'Arial, sans-serif'}\n",
    "    },\n",
    "    font=dict(size=16, family='Arial, sans-serif'),\n",
    "    plot_bgcolor='rgba(0,0,0,0)',    # transparent plotting area\n",
    "    paper_bgcolor='rgba(0,0,0,0)',   # transparent paper/background\n",
    "    width=1800,\n",
    "    height=1200,\n",
    "    margin=dict(l=50, r=50, t=80, b=50)\n",
    ")\n",
    "\n",
    "# TFs title in orange\n",
    "fig.add_annotation(dict(\n",
    "    x=0,\n",
    "    y=0.95,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    text='TFs',\n",
    "    showarrow=False,\n",
    "    font=dict(size=24, family='Arial, sans-serif',\n",
    "              color='rgba(255,127,14,1)')  # orange\n",
    "))\n",
    "\n",
    "# Enzymes title in green\n",
    "fig.add_annotation(dict(\n",
    "    x=0.5,\n",
    "    y=1.03,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    text='Enzymes',\n",
    "    showarrow=False,\n",
    "    font=dict(size=24, family='Arial, sans-serif',\n",
    "              color='rgba(44,160,44,1)')   # green\n",
    "))\n",
    "\n",
    "# Pathways title in blue\n",
    "fig.add_annotation(dict(\n",
    "    x=1.015,\n",
    "    y=1.03,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    text='Pathways',\n",
    "    showarrow=False,\n",
    "    font=dict(size=24, family='Arial, sans-serif',\n",
    "              color='rgba(31,119,180,1)')  # blue\n",
    "))\n",
    "\n",
    "# # Save the figure to pdf\n",
    "# fig.write_image(\"sankey_plot.png\", format=\"png\", scale=3)\n",
    "\n",
    "# # Save the figure as pdf\n",
    "# fig.write_image(\"../../results/physiology_comparison/sankey_plot.pdf\", format=\"pdf\", scale=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
